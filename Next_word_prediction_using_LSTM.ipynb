{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Z4HRA-S/NLP_Course_Spring2023/blob/main/Next_word_prediction_using_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6-NPLt4etsb"
      },
      "source": [
        "#Next Word Prediction Using LSTM\n",
        "In this session, we want to predict the next token based on a sequence. For more detail, see this [link](https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/).\n",
        " This task is charachter-level. It means our model will predict the next character :)\n",
        "bold text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3LTDH3MMWEs_"
      },
      "outputs": [],
      "source": [
        "# Small LSTM Network to Generate Text for Alice in Wonderland\n",
        "import numpy as np\n",
        "import nltk\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import Embedding\n",
        "from keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define a function for cleaning our text."
      ],
      "metadata": {
        "id": "XeB64B0k2Geo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vz0mgk5O9VwC",
        "outputId": "3073c3a7-25ec-4d3f-8a8d-2a0f5beb3c6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "\"\"\"def clean_text(text:str)->list:\n",
        "    data = word_tokenize(text)\n",
        "    data = [word.lower() for word in data]\n",
        "    data = [d.translate(str.maketrans('', '', string.punctuation)) for d in data]\n",
        "    data = [d for d in data if d.isalpha()]\n",
        "    return data\"\"\"\n",
        "\n",
        "\n",
        "def clean_text_char_level(data:str)->list:\n",
        "    data = data.lower()\n",
        "    data = data.translate(str.maketrans('', '', string.punctuation))\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We download the brown corpus from nltk. "
      ],
      "metadata": {
        "id": "vbI_kOsV2MDH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlYkNEHbWSFy",
        "outputId": "c642209d-7d1b-4f46-bb74-c3ab000673fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import brown\n",
        "import nltk\n",
        "\n",
        "nltk.download('brown')\n",
        "\n",
        "data=brown.sents(categories=['news','reviews'])\n",
        "data = \" \".join(sum(data,[]))\n",
        "data = clean_text_char_level(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our vocab includes english chars."
      ],
      "metadata": {
        "id": "Zc2zWl4Y2YCn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNiLVUbZPWEF",
        "outputId": "e18028f9-b5af-4f66-e372-7546f792d361"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37\n"
          ]
        }
      ],
      "source": [
        "vocab = set(data)\n",
        "print(len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXkdCjsVPcfE",
        "outputId": "9aa43297-5f58-472d-bb41-e2bfc7494143"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from collections import Counter\\nvocab_count = dict(Counter(data))\\nlow_freq_words = [k for k,v in vocab_count.items() if v<2]\\nprint(\"words with one accourance in data: \", len(low_freq_words))'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "\"\"\"from collections import Counter\n",
        "vocab_count = dict(Counter(data))\n",
        "low_freq_words = [k for k,v in vocab_count.items() if v<2]\n",
        "print(\"words with one accourance in data: \", len(low_freq_words))\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0xFmwW0Qcyi",
        "outputId": "7e45fcbf-dccc-4972-adb9-dcfd888ec04a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from nltk.corpus import words\\nnltk.download(\"words\")\\n\\nvalid_low_freq_words = list(filter(lambda x: x in words.words(), low_freq_words))'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\"\"\"from nltk.corpus import words\n",
        "nltk.download(\"words\")\n",
        "\n",
        "valid_low_freq_words = list(filter(lambda x: x in words.words(), low_freq_words))\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EIAnHvsYJQm",
        "outputId": "aca4d8e8-0518-4ae7-ac91-08ad86ab725e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'len(valid_low_freq_words )'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "\"\"\"len(valid_low_freq_words )\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLjUYf83X8IG",
        "outputId": "a850ab54-af0e-434d-dd90-6b57bc50b97a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'vocab=[k for k,v in vocab_count.items() if v>=3]\\nvocab.append(\"unk\")\\nvocab=set(vocab)\\n#vocab.extend(valid_low_freq_words)\\nlen(vocab)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "\"\"\"vocab=[k for k,v in vocab_count.items() if v>=3]\n",
        "vocab.append(\"unk\")\n",
        "vocab=set(vocab)\n",
        "#vocab.extend(valid_low_freq_words)\n",
        "len(vocab)\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define word_to_id as a mapping between chars and numbers."
      ],
      "metadata": {
        "id": "Pvz7q41K2d5-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Pi76xjDuYXr2"
      },
      "outputs": [],
      "source": [
        "\"\"\"tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(set(vocab))\n",
        "sequences = tokenizer.texts_to_sequences(data)\"\"\"\n",
        "word_to_id = {k:v for v,k in enumerate(vocab)}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next cell, we take 70-length sequence of chars of our corpus, and we take the 71th char as the sequence's label."
      ],
      "metadata": {
        "id": "vTqNCWR62lMl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zAwjE3Ja2SZ",
        "outputId": "15c41109-0853-4894-cd79-0ba1d207cb8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Patterns:  736304\n"
          ]
        }
      ],
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 70\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, len(data) - seq_length, 1):\n",
        "\tseq_in = data[i:i + seq_length]\n",
        "\tseq_out = data[i + seq_length]\n",
        "\tdataX.append([word_to_id[s] for s in seq_in])\n",
        "\tdataY.append(word_to_id[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbS0OuY0JW4r",
        "outputId": "a08cffd8-bffe-41a1-bc27-21b5854bc585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[21, 27, 8, 32, 31, 2, 9, 21, 28, 11, 17, 31, 1, 21, 5, 32, 11, 9, 21, 14, 17, 3, 1, 21, 30, 11, 3, 1, 17, 9, 21, 17, 31, 21, 3, 31, 18, 16, 14, 2, 3, 28, 17, 2, 3, 8, 31, 21, 8, 30, 21, 17, 2, 24, 17, 31, 2, 17, 14, 21, 11, 16, 27, 16, 31, 2, 21, 12, 11, 3]\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "print(dataX[10])\n",
        "print(dataY[10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcAA-_g8ifj5",
        "outputId": "e5651dc3-be80-4ebd-92bf-102986fd2d5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37\n"
          ]
        }
      ],
      "source": [
        "\"\"\"vocab_size = len(tokenizer.word_index) + 1\"\"\"\n",
        "vocab_size = len(vocab)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next cell, we normalize the input to the model to be in the range of (0,1) and turn the label into one-hot vectors."
      ],
      "metadata": {
        "id": "I2WJERYN27OV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwBYOfQCJvND",
        "outputId": "f5ee4265-c993-4a09-c75a-8f3e28448593"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(736304, 70, 1) (736304, 37)\n"
          ]
        }
      ],
      "source": [
        "dataX=np.array(dataX)/float(len(vocab))\n",
        "dataY=np.array(dataY)\n",
        "dataX=np.reshape(dataX, (dataX.shape[0], seq_length, 1))\n",
        "dataY = to_categorical(dataY, num_classes=vocab_size)\n",
        "\n",
        "print(dataX.shape, dataY.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next cell, we define a deep neural network model. Don't worry if you did not take the deep learning course, just run the cell. "
      ],
      "metadata": {
        "id": "xaGrvINV3YS_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "25tCnlDVdLju"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Flatten\n",
        "import tensorflow as tf\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(seq_length,return_sequences=True,input_shape=(dataX.shape[1],1)))\n",
        "#model.add(Dropout(0.1))\n",
        "model.add(LSTM(seq_length,return_sequences=True))\n",
        "#model.add(Dropout(0.1))\n",
        "model.add(LSTM(seq_length))\n",
        "model.add(Dense(dataY.shape[1], activation='softmax'))\n",
        "optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiI3mwZlk-mu",
        "outputId": "186d1833-09f4-45c1-b056-39117cf125f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 70, 70)            20160     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 70, 70)            39480     \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 70)                39480     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 37)                2627      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 101,747\n",
            "Trainable params: 101,747\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training will occur in the next cell, it may take a while. Go make a cup of tea for yourself 🫖 🍵"
      ],
      "metadata": {
        "id": "q7CRJ8ii3syH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2EfO8XtKJ7v",
        "outputId": "25c9e093-5916-4467-f4ee-57469ecf3692"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "4244/5753 [=====================>........] - ETA: 11:23 - loss: 2.7707 - accuracy: 0.2146"
          ]
        }
      ],
      "source": [
        "model.fit(dataX, dataY, epochs=30, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training the model, we will evaluate our model. If you want to test it, define your desired text."
      ],
      "metadata": {
        "id": "QgMRcyCA4KnB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAhePETZvJZC"
      },
      "outputs": [],
      "source": [
        "text= \"The 17th-century British empiricist John Locke held that linguistic meaning is mental: words are used to encode and convey thoughts, or ideas. Successful communication requires that the hearer correctly decode the speaker’s words into their associated ideas. So construed, the meaning of an expression, according to Locke, is the idea associated with it in the mind of anyone who knows and understands that expression.\"\n",
        "text = clean_text_char_level(data)\n",
        "\n",
        "model_input = [word_to_id[char] for char in text]\n",
        "\n",
        "output_vector = [model.predict(model_input[i:i+10]) for i in range(len(text)-seq_length)]\n",
        "idx = [np.argmax(v) for v in output_vector]\n",
        "\n",
        "id_to_char={v:k for k,v in word_to_id.items()}\n",
        "\n",
        "output = [id_to_char(s) for s in idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Home Work\n",
        "These homework are subjective, and your way of thinking and researching is more important than your actual answers (but pay attention to your actual answers too :) ). Try to think about the problem, and if you discuss it with your friends, please include their names in your email.\n",
        "\n",
        "1. Run the notebook and ask any question, or discuss your ideas and thoughts. \n",
        "2. Test some other text with the trained model and see the answers. \n",
        "3. Think about the pre-processing and see if you can do it better. If you came up with any ideas, try it and submit it in  your email.\n",
        "4. Explore the data. Search and see if you can run the model with another dataset. compare your data set with the current dataset and include the result of the training, including accuracy and loss. \n"
      ],
      "metadata": {
        "id": "tkivhOMp5bhi"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNm00ymLDkjZI5irB3rMu+G",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}