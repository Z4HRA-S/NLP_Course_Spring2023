{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPC9gOafi7mvNGTVw3KHIx8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Z4HRA-S/NLP_Course_Spring2023/blob/main/NLTK_nlp_spring2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to NLTK \n",
        "###Spring 2023 | Session 1 \n",
        "\n",
        "The goal of this session is to get familiar with the `NLTK` package and some of the text pre-processing steps. \n",
        "We will cover the following concepts: \n",
        "* Tokenization\n",
        "* Stemming\n",
        "* Lemmatizing\n",
        "* POS tagging"
      ],
      "metadata": {
        "id": "q6ZjYypbko3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use a Tolkien's Poem for our sample text :) \n",
        "\n",
        "my_string = \"\"\"All that is gold does not glitter,\n",
        "Not all those who wander are lost;\n",
        "The old that is strong does not wither,\n",
        "Deep roots are not reached by the frost.\n",
        "From the ashes a fire shall be woken,\n",
        "A light from the shadows shall spring;\n",
        "Renewed shall be blade that was broken,\n",
        "The crownless again shall be king.\"\"\""
      ],
      "metadata": {
        "id": "q1kMyMFKCyxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will try two of built-in python's function for strings."
      ],
      "metadata": {
        "id": "1uUsrEHgloHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_string.split(\"\\n\")"
      ],
      "metadata": {
        "id": "fHEznsluC58u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_string.split(\" \")"
      ],
      "metadata": {
        "id": "pMjueEkdC89J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_string.replace(\"shall\",\"***will***\"))"
      ],
      "metadata": {
        "id": "HP6_8g9FDAPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###NLTK"
      ],
      "metadata": {
        "id": "HCKiV86KlxFY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq0s8-Km_sZO",
        "outputId": "8203b163-8cbc-469a-d897-5e0dc6c83510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenization\n",
        "\n",
        "Token's definition by [Stanford NLP course](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html): \"Tokens are often loosely referred to as terms or words, but it is sometimes important to make a type/token distinction. A token is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing.\"\n",
        "\n",
        "What is Tokenization? An answer by [Anni Burchfiel](https://www.tokenex.com/blog/ab-what-is-nlp-natural-language-processing-tokenization/): \"Tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning. The first step of the NLP process is gathering the data (a sentence) and breaking it into understandable parts (words).\""
      ],
      "metadata": {
        "id": "Sz5qJ4c4CXFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(my_string)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5u-hmSw_2-m",
        "outputId": "7a358093-c92d-4c89-9cc6-3c3e31044021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['All', 'that', 'is', 'gold', 'does', 'not', 'glitter', ',', 'Not', 'all', 'those', 'who', 'wander', 'are', 'lost', ';', 'The', 'old', 'that', 'is', 'strong', 'does', 'not', 'wither', ',', 'Deep', 'roots', 'are', 'not', 'reached', 'by', 'the', 'frost', '.', 'From', 'the', 'ashes', 'a', 'fire', 'shall', 'be', 'woken', ',', 'A', 'light', 'from', 'the', 'shadows', 'shall', 'spring', ';', 'Renewed', 'shall', 'be', 'blade', 'that', 'was', 'broken', ',', 'The', 'crownless', 'again', 'shall', 'be', 'king', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [t.lower() for t in tokens]"
      ],
      "metadata": {
        "id": "4OuoLwbICVVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###N_grams\n",
        "N-grams are a continuous slice of a textual sequence. The N refers to the slice size (N tokens, symbols, words, etc)."
      ],
      "metadata": {
        "id": "rj8Xcg3XDzK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "\n",
        "[a for a in ngrams(tokens, 4, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_')]"
      ],
      "metadata": {
        "id": "Fzjj0Bj8Dwlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Stemming\n",
        "\n",
        "Stemming is a process by which word endings or other affixes are removed or modified in order that word forms which differ in non-relevant ways may be merged and treated as equivalent. [link](https://link.springer.com/referenceworkentry/10.1007/978-1-4899-7993-3_942-2)"
      ],
      "metadata": {
        "id": "OWOXgN-VHc_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import (PorterStemmer, LancasterStemmer)\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "\n",
        "print([porter.stem(t) for t in tokens])\n",
        "print([lancaster.stem(t) for t in tokens])\n",
        "print([snowball.stem(t) for t in tokens])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Csq9Sa-FBGJ",
        "outputId": "9bcb0cb3-8182-4f70-b4e3-a49cb27765fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['all', 'that', 'is', 'gold', 'doe', 'not', 'glitter', ',', 'not', 'all', 'those', 'who', 'wander', 'are', 'lost', ';', 'the', 'old', 'that', 'is', 'strong', 'doe', 'not', 'wither', ',', 'deep', 'root', 'are', 'not', 'reach', 'by', 'the', 'frost', '.', 'from', 'the', 'ash', 'a', 'fire', 'shall', 'be', 'woken', ',', 'a', 'light', 'from', 'the', 'shadow', 'shall', 'spring', ';', 'renew', 'shall', 'be', 'blade', 'that', 'wa', 'broken', ',', 'the', 'crownless', 'again', 'shall', 'be', 'king', '.']\n",
            "['al', 'that', 'is', 'gold', 'doe', 'not', 'glit', ',', 'not', 'al', 'thos', 'who', 'wand', 'ar', 'lost', ';', 'the', 'old', 'that', 'is', 'strong', 'doe', 'not', 'with', ',', 'deep', 'root', 'ar', 'not', 'reach', 'by', 'the', 'frost', '.', 'from', 'the', 'ash', 'a', 'fir', 'shal', 'be', 'wok', ',', 'a', 'light', 'from', 'the', 'shadow', 'shal', 'spring', ';', 'renew', 'shal', 'be', 'blad', 'that', 'was', 'brok', ',', 'the', 'crownless', 'again', 'shal', 'be', 'king', '.']\n",
            "['all', 'that', 'is', 'gold', 'doe', 'not', 'glitter', ',', 'not', 'all', 'those', 'who', 'wander', 'are', 'lost', ';', 'the', 'old', 'that', 'is', 'strong', 'doe', 'not', 'wither', ',', 'deep', 'root', 'are', 'not', 'reach', 'by', 'the', 'frost', '.', 'from', 'the', 'ash', 'a', 'fire', 'shall', 'be', 'woken', ',', 'a', 'light', 'from', 'the', 'shadow', 'shall', 'spring', ';', 'renew', 'shall', 'be', 'blade', 'that', 'was', 'broken', ',', 'the', 'crownless', 'again', 'shall', 'be', 'king', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Lemmatizing\n",
        "\n",
        "\n",
        "Lemmatization is a process of determining a base or dictionary form (lemma) for a given surface form. Especially for languages with rich morphology it is important to be able to normalize words into their base forms to better support for example search engines and linguistic studies. [link](https://arxiv.org/abs/1902.00972)"
      ],
      "metadata": {
        "id": "Dk34oeg2J9Mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk import WordNetLemmatizer\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "print([wnl.lemmatize(t, pos=\"v\") for t in tokens])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moMtxDgLJ8MA",
        "outputId": "745feddc-b262-419e-90b5-e9db1eea27e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###POS\n",
        "\"Part-of-speech (POS) tagging, also called grammatical tagging, is the automatic assignment of part-of-speech tags to words in a sentence. A POS is a grammatical classification that commonly includes verbs, adjectives, adverbs, nouns, etc.\" [Chiche, A. 2022] [link](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00561-y) \n",
        "\n",
        "\n",
        "|Tag |\tMeaning |\tEnglish Examples|\n",
        "|----|----------|-----------------|\n",
        "ADJ |\tadjective \t|new, good, high, special, big, local\n",
        "ADP \t|adposition \t|on, of, at, with, by, into, under\n",
        "ADV |\tadverb| \treally, already, still, early, now\n",
        "CONJ |\tconjunction |\tand, or, but, if, while, although\n",
        "DET \t|determiner, article |\tthe, a, some, most, every, no, which\n",
        "NOUN \t|noun |\tyear, home, costs, time, Africa\n",
        "NUM \t|numeral \t|twenty-four, fourth, 1991, 14:24\n",
        "PRT \t|particle |\tat, on, out, over per, that, up, with\n",
        "PRON \t|pronoun \t|he, their, her, its, my, I, us\n",
        "VERB \t|verb |\tis, say, told, given, playing, would\n",
        ". \t|punctuation marks |\t. , ; !\n",
        "X \t|other |\tersatz, esprit, dunno, gr8, univeristy"
      ],
      "metadata": {
        "id": "IWfFLc4rLC04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"tagsets\")\n",
        "#nltk.help.upenn_tagset()\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.pos_tag(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upxqjNkrLB1u",
        "outputId": "5572533a-5ae8-455c-e5fd-85d4af4fdd0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('all', 'DT'),\n",
              " ('that', 'DT'),\n",
              " ('is', 'VBZ'),\n",
              " ('gold', 'NN'),\n",
              " ('does', 'VBZ'),\n",
              " ('not', 'RB'),\n",
              " ('glitter', 'VB'),\n",
              " (',', ','),\n",
              " ('not', 'RB'),\n",
              " ('all', 'PDT'),\n",
              " ('those', 'DT'),\n",
              " ('who', 'WP'),\n",
              " ('wander', 'VBP'),\n",
              " ('are', 'VBP'),\n",
              " ('lost', 'VBN'),\n",
              " (';', ':'),\n",
              " ('the', 'DT'),\n",
              " ('old', 'JJ'),\n",
              " ('that', 'WDT'),\n",
              " ('is', 'VBZ'),\n",
              " ('strong', 'JJ'),\n",
              " ('does', 'VBZ'),\n",
              " ('not', 'RB'),\n",
              " ('wither', 'VB'),\n",
              " (',', ','),\n",
              " ('deep', 'JJ'),\n",
              " ('roots', 'NNS'),\n",
              " ('are', 'VBP'),\n",
              " ('not', 'RB'),\n",
              " ('reached', 'VBN'),\n",
              " ('by', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('frost', 'NN'),\n",
              " ('.', '.'),\n",
              " ('from', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('ashes', 'NNS'),\n",
              " ('a', 'DT'),\n",
              " ('fire', 'NN'),\n",
              " ('shall', 'MD'),\n",
              " ('be', 'VB'),\n",
              " ('woken', 'VBN'),\n",
              " (',', ','),\n",
              " ('a', 'DT'),\n",
              " ('light', 'NN'),\n",
              " ('from', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('shadows', 'NNS'),\n",
              " ('shall', 'MD'),\n",
              " ('spring', 'NN'),\n",
              " (';', ':'),\n",
              " ('renewed', 'VBN'),\n",
              " ('shall', 'MD'),\n",
              " ('be', 'VB'),\n",
              " ('blade', 'VBN'),\n",
              " ('that', 'DT'),\n",
              " ('was', 'VBD'),\n",
              " ('broken', 'VBN'),\n",
              " (',', ','),\n",
              " ('the', 'DT'),\n",
              " ('crownless', 'NN'),\n",
              " ('again', 'RB'),\n",
              " ('shall', 'MD'),\n",
              " ('be', 'VB'),\n",
              " ('king', 'VBG'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want an informative description of each tag, you can use the code below."
      ],
      "metadata": {
        "id": "4E-lMAsLn6Mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.help.upenn_tagset('VBN')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVgjaPBFN2sc",
        "outputId": "1fffd2ea-b2ac-48a2-a704-debbf56b0077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VBN: verb, past participle\n",
            "    multihulled dilapidated aerosolized chaired languished panelized used\n",
            "    experimented flourished imitated reunifed factored condensed sheared\n",
            "    unsettled primed dubbed desired ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Homework\n",
        "\n",
        "1. Load the `Harry_potter_and_the_order_of_the_phoenix_chapter_1.txt` file from the `data` directory. Write a function to process the whole text and return one dictionary with the **verb's** root as the key and the list of occurrences of the **key-verb** as value. For example for this sentence: \n",
        " \"*The injustice of it all burst inside him so that he wanted to yell with fury, and he wanted to tell the whole world.*\" we would have this dictionary: \n",
        "```\n",
        "{\"want\":[11,19], \"yell\":[13], \"burst\":[5],\"tell\":[21]}\n",
        "```\n",
        "We consider the word position here. \n",
        "\n",
        "2. Write another program to take a verb from the user and search in the dictionary you made in Q.1 and return those sentences containing the verb or another form of the verb.\n",
        "\n",
        "\n",
        "3. Search about the Stop Word, and answer these questions: \n",
        "* What are the Stop Words? \n",
        "* In what kind of situation do they act as noise in our data and when do they help to have better results?\n",
        "* Can we remove the stop words using NLTK? \n",
        "\n",
        "4. We have 3 different texts, one is some of Shakespeare's poems `shakespeare.txt`, one is a scientific article from Wikipedia -`cognitive_revolution.txt`- and the other is one chapter of the Harry Potter book series `Harry_potter_and_the_order_of_the_phoenix_chapter_1.txt`.\n",
        "\n",
        " We want to calculate the readability of these texts. It means we want to know how easy each text is to understand. One of the most used readability measures is [flesch reading ease metric](https://readable.com/readability/flesch-reading-ease-flesch-kincaid-grade-level/) which is calculated based on the average number of words in a sentence and the average number of syllables in words. It's formula is: \n",
        "$206.835 - 1.015(\\frac{\\text{total words}}{\\text{total sentences}}) - 84.6(\\frac{\\text{total syllables}}{\\text{total words}})$\n",
        "\n",
        "calculate the readability measure for these 3 texts.\n",
        "\n",
        "Hint: The `nltk.tokenize.sent_tokenize` can be used for splitting the whole text into sentences. For `shakespeare.txt` you can use splitting by `\\n` to have each line as a separate sentence. \n",
        "\n",
        "\n",
        "______________________\n",
        "\n",
        "\n",
        "**Submit the code and the sample result for Q.1 and Q.2 and the result for Q.3 and Q.4 by 11-2-1402 at midnight. Please send an email to z.sarlak@iasbs.ac.ir with the title \"NLTK_HW1\".**"
      ],
      "metadata": {
        "id": "rzi9PRu7-orW"
      }
    }
  ]
}